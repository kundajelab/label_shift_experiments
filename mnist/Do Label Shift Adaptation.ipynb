{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import abstention\n",
    "reload(abstention)\n",
    "reload(abstention.calibration)\n",
    "reload(abstention.label_shift)\n",
    "from abstention.calibration import TempScaling, ConfusionMatrix, softmax\n",
    "from abstention.label_shift import EMImbalanceAdapter, BBSEImbalanceAdapter, ShiftWeightFromImbalanceAdapter\n",
    "import glob\n",
    "import gzip\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_labels(fh):\n",
    "    to_return = []\n",
    "    for line in fh:\n",
    "        the_class=int(line.rstrip())\n",
    "        to_add = np.zeros(10)\n",
    "        to_add[the_class] = 1\n",
    "        to_return.append(to_add)\n",
    "    return np.array(to_return)\n",
    "\n",
    "def read_preds(fh):\n",
    "    return np.array([[float(x) for x in y.decode(\"utf-8\").rstrip().split(\"\\t\")]\n",
    "                     for y in fh])\n",
    "\n",
    "def sample_from_probs_arr(arr_with_probs):\n",
    "    rand_num = np.random.random()\n",
    "    cdf_so_far = 0\n",
    "    for (idx, prob) in enumerate(arr_with_probs):\n",
    "        cdf_so_far += prob\n",
    "        if (cdf_so_far >= rand_num\n",
    "            or idx == (len(arr_with_probs) - 1)):  # need the\n",
    "            # letterIdx==(len(row)-1) clause because of potential floating point errors\n",
    "            # that mean arrWithProbs doesn't sum to 1\n",
    "            return idx\n",
    "        \n",
    "test_labels = read_labels(gzip.open(glob.glob(\"test_labels.txt.gz\")[0]))\n",
    "test_class_to_indices = defaultdict(list)\n",
    "for index,row in enumerate(test_labels):\n",
    "    row_label = np.argmax(row)\n",
    "    test_class_to_indices[row_label].append(index)\n",
    "def draw_test_indices(total_to_return, label_proportions):\n",
    "    indices_to_use = []\n",
    "    for class_index, class_proportion in enumerate(label_proportions):\n",
    "        indices_to_use.extend(np.random.choice(\n",
    "                test_class_to_indices[class_index],\n",
    "                int(total_to_return*class_proportion),\n",
    "                replace=True))\n",
    "    for i in range(total_to_return-len(indices_to_use)):\n",
    "        class_index = sample_from_probs_arr(label_proportions)\n",
    "        indices_to_use.append(\n",
    "            np.random.choice(test_class_to_indices[class_index]))\n",
    "    return indices_to_use\n",
    "\n",
    "valid_labels = read_labels(gzip.open(glob.glob(\"valid_labels.txt.gz\")[0]))\n",
    "\n",
    "imbalance_adapters = [\n",
    "    ('em_calib-confusionmat_init-default', EMImbalanceAdapter(calibrator_factory=ConfusionMatrix(), verbose=False)),\n",
    "    ('em_calib-confusionmat_init-BBSE-hard', EMImbalanceAdapter(calibrator_factory=ConfusionMatrix(), verbose=False,\n",
    "                                                                    initialization_weight_ratio=\n",
    "                                                                     ShiftWeightFromImbalanceAdapter(BBSEImbalanceAdapter(soft=False)))),\n",
    "    ('em_calib-None_init-default', EMImbalanceAdapter(calibrator_factory=None)),\n",
    "    ('em_calib-tsnobiascorr_init-default', EMImbalanceAdapter(calibrator_factory=TempScaling(verbose=False))),\n",
    "    ('em_calib-tswithbiascorr_init-default', EMImbalanceAdapter(calibrator_factory=\n",
    "                                                       TempScaling(verbose=False,bias_positions=[0,1,2,3,4,5,6,7,8,9]))),\n",
    "    ('bbse-hard_calib-None', BBSEImbalanceAdapter(soft=False, calibrator_factory=None)),\n",
    "    ('bbse-soft_calib-None', BBSEImbalanceAdapter(soft=True, calibrator_factory=None)),\n",
    "    ('bbse-soft_calib-tsnobiascorr', BBSEImbalanceAdapter(soft=True, calibrator_factory=TempScaling(verbose=False))),\n",
    "    ('bbse-hard_calib-tsnobiascorr', BBSEImbalanceAdapter(soft=False, calibrator_factory=TempScaling(verbose=False))),\n",
    "    ('bbse-soft_calib-tswithbiascorr', BBSEImbalanceAdapter(soft=True, calibrator_factory=TempScaling(verbose=False,\n",
    "                                                                                      bias_positions=[0,1,2,3,4,5,6,7,8,9]))), \n",
    "    ('bbse-hard_calib-tswithbiascorr', BBSEImbalanceAdapter(soft=False, calibrator_factory=TempScaling(verbose=False,\n",
    "                                                                                      bias_positions=[0,1,2,3,4,5,6,7,8,9]))),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals = defaultdict(\n",
    "                                                                  lambda: defaultdict(\n",
    "                                                                           lambda: defaultdict(\n",
    "                                                                                    lambda: defaultdict(list))))\n",
    "dirichletalpha_to_samplesize_to_baselineacc = defaultdict(lambda: defaultdict(list))\n",
    "num_trials = 10\n",
    "dirichlet_alphas_and_samplesize = [(0.01, 500), (0.1,500), (1.0,500), (10.0,500),\n",
    "                                   (0.01, 1000), (0.1,1000), (1.0,1000), (10.0,1000),\n",
    "                                   (0.01, 2000), (0.1,2000), (1.0,2000), (10.0,2000),\n",
    "                                   (0.01, 4000), (0.1,4000), (1.0,4000), (10.0,4000),\n",
    "                                   (0.01, 8000), (0.1,8000), (1.0,8000), (10.0,8000),]\n",
    "for (dirichlet_alpha,samplesize) in dirichlet_alphas_and_samplesize:\n",
    "    #for model_idx,train_set_size in enumerate([250, 500, 1000, 2000, 4000, 8000, 16000]):\n",
    "    for seed in [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "        print(\"Seed\",seed)\n",
    "        test_preds = softmax(preact=read_preds(gzip.open(glob.glob(\"cifar10_balanced_seed-\"+str(seed)+\"_*testpreds.txt.gz\")[0])),\n",
    "                             temp=1, biases=None)\n",
    "        valid_preds = softmax(preact=read_preds(gzip.open(glob.glob(\"cifar10_balanced_seed-\"+str(seed)+\"_*validpreds.txt.gz\")[0])),\n",
    "                              temp=1, biases=None)\n",
    "        sample_valid_preds = valid_preds[:samplesize]\n",
    "        sample_valid_labels = valid_labels[:samplesize]\n",
    "        for trial_num in range(num_trials):\n",
    "            #print(\"On trial num\",trial_num)\n",
    "            sys.stdout.flush()\n",
    "            np.random.seed(trial_num*100)\n",
    "            random.seed(trial_num*100)\n",
    "            dirichlet_dist = np.random.dirichlet([dirichlet_alpha for x in range(10)])\n",
    "            test_indices = draw_test_indices(total_to_return=samplesize,\n",
    "                                             label_proportions=dirichlet_dist)\n",
    "            shifted_test_labels = test_labels[test_indices]\n",
    "            shifted_test_preds = test_preds[test_indices]\n",
    "            \n",
    "            shifted_test_baseline_accuracy = np.mean(np.argmax(shifted_test_labels,axis=-1)==\n",
    "                                                     np.argmax(shifted_test_preds,axis=-1))\n",
    "            dirichletalpha_to_samplesize_to_baselineacc[dirichlet_alpha][samplesize].append(shifted_test_baseline_accuracy)\n",
    "            \n",
    "            ideal_shift_weights = np.mean(shifted_test_labels,axis=0)/np.mean(sample_valid_labels,axis=0)\n",
    "            for adapter_name,imbalance_adapter in imbalance_adapters:\n",
    "                #print(adapter_name)\n",
    "                imbalance_adapter_func = imbalance_adapter(valid_labels=sample_valid_labels,\n",
    "                                                           tofit_initial_posterior_probs=shifted_test_preds,\n",
    "                                                           valid_posterior_probs=sample_valid_preds)  \n",
    "                shift_weights = imbalance_adapter_func.multipliers\n",
    "                adapted_shifted_test_preds = imbalance_adapter_func(shifted_test_preds)\n",
    "                adapted_shifted_test_accuracy = np.mean(np.argmax(shifted_test_labels,axis=-1)==\n",
    "                                                        np.argmax(adapted_shifted_test_preds,axis=-1))\n",
    "                delta_from_baseline = adapted_shifted_test_accuracy-shifted_test_baseline_accuracy\n",
    "                \n",
    "                dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals[\n",
    "                    dirichlet_alpha][samplesize][adapter_name]['weightdiffnorm'].append(\n",
    "                    np.linalg.norm(shift_weights-ideal_shift_weights))\n",
    "                dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals[\n",
    "                    dirichlet_alpha][samplesize][adapter_name]['delta_acc'].append(\n",
    "                    delta_from_baseline)\n",
    "                    \n",
    "    print(\"On alpha\",dirichlet_alpha,\"sample size\", samplesize)\n",
    "    for metric_name in ['delta_acc', 'weightdiffnorm']:\n",
    "        print(\"Metric\",metric_name)\n",
    "        for adapter_name in [x[0] for x in imbalance_adapters]:\n",
    "            n = len(dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals[\n",
    "                                dirichlet_alpha][samplesize][adapter_name][metric_name])\n",
    "            \n",
    "            print(adapter_name,\n",
    "                  np.mean(dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals[\n",
    "                            dirichlet_alpha][samplesize][adapter_name][metric_name]),\n",
    "                  \"+/-\",\n",
    "                  (1.0/np.sqrt(n))*np.std(dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals[\n",
    "                                  dirichlet_alpha][samplesize][adapter_name][metric_name],\n",
    "                                 ddof=1))\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "file_out = \"label_shift_adaptation_results.json\"\n",
    "dict_to_write = {\n",
    "    \"dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals\":\n",
    "     dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals,\n",
    "    \"dirichletalpha_to_samplesize_to_baselineacc\": dirichletalpha_to_samplesize_to_baselineacc,\n",
    "}\n",
    "open(file_out, 'w').write(\n",
    "    json.dumps(dict_to_write,\n",
    "               sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "os.system(\"gzip -f \"+file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "loaded_dicts = json.loads(gzip.open(\"label_shift_adaptation_results.json.gz\").read())\n",
    "dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals =\\\n",
    "    loaded_dicts['dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals']\n",
    "dirichletalpha_to_samplesize_to_baselineacc = loaded_dicts['dirichletalpha_to_samplesize_to_baselineacc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from abstention.figure_making_utils import (\n",
    "    wilcox_srs, get_ustats_mat,\n",
    "    get_tied_top_and_worst_methods)\n",
    "from scipy.stats import norm\n",
    "\n",
    "#columns are: method type, calibration strategy alpha perfs\n",
    "methods_to_consider = [\n",
    "        'bbse-hard_calib-None',\n",
    "        'bbse-soft_calib-None',\n",
    "        'em_calib-None_init-default',\n",
    "        'bbse-soft_calib-tsnobiascorr',\n",
    "        'bbse-soft_calib-tswithbiascorr',        \n",
    "        'em_calib-tsnobiascorr_init-default',\n",
    "        'em_calib-tswithbiascorr_init-default',\n",
    "]\n",
    "our_proposed_methods = set([\n",
    "           'em_calib-tswithbiascorr_init-default',\n",
    "           'em_calib-tsnobiascorr_init-default',\n",
    "           #'em_calib-None_init-default',\n",
    "           #'bbse-hard_calib-None',\n",
    "           #'bbse-soft_calib-None',\n",
    "           'bbse-soft_calib-tsnobiascorr',\n",
    "           'bbse-soft_calib-tswithbiascorr'])\n",
    "\n",
    "metrics = [\"weightdiffnorm\", \"delta_acc\"]\n",
    "#metrics = [\"weightdiffnorm\"]\n",
    "metric_to_nicename = {'delta_acc': \"$\\\\bm{\\\\Delta}$\\\\textbf{\\\\%Accuracy}\",\n",
    "                      'weightdiffnorm': \"$\\\\bm{| w - \\hat{w} |}$\"}\n",
    "adaptmethod_to_nicename = {'em': 'EM',\n",
    "                           'bbse-soft': 'BBSE-soft',\n",
    "                           'bbse-hard': 'BBSE-hard'}\n",
    "calibmethod_to_nicename = {'None': 'None',\n",
    "                           'tsnobiascorr': 'Temp. Scale',\n",
    "                           'tswithbiascorr': 'B.C. Temp. Scale'}\n",
    "metric_to_largerisbetter = {'delta_acc':True, 'weightdiffnorm': False}\n",
    "\n",
    "sets = [\n",
    "    (\" \\\\textbf{under different} $\\\\bm{\\\\alpha}$ \", [('0.01', '8000'), ('0.1', '8000'), ('1.0', '8000')],\n",
    "      'alpha'),\n",
    "    (\" \\\\textbf{under different} $\\\\bm{n}$ \", [\n",
    "            ('0.1', '500'), #('0.1', '1000'),\n",
    "            ('0.1', '2000'),# ('0.1', '4000'),\n",
    "            ('0.1', '8000')], 'n')\n",
    "]\n",
    "\n",
    "for metric in metrics:\n",
    "    for set_name, set_cols, varyingparam in sets:\n",
    "    #print(\"Set\",set_name)\n",
    "    \n",
    "        #print(\"Metric:\",metric)\n",
    "        \n",
    "        condition_to_best_methods = {}\n",
    "        for alpha,samplesize in set_cols:\n",
    "            method_to_perfs = dict([(method_name,\n",
    "                                    dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals[alpha][samplesize][method_name][metric])\n",
    "                                    for method_name in methods_to_consider])\n",
    "            ustats_mat = get_ustats_mat(\n",
    "                method_to_perfs=method_to_perfs,\n",
    "                method_names=methods_to_consider,\n",
    "                max_ustat=(101*50))\n",
    "            #print(ustats_mat)\n",
    "            tied_top_methods, tied_worst_methods =(\n",
    "                get_tied_top_and_worst_methods(\n",
    "                    ustats_mat=ustats_mat,\n",
    "                    method_names=methods_to_consider,\n",
    "                    #Using the normal approximation at N=100;\n",
    "                    # variance from https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\n",
    "                    #Note that T = ((N+1)*N/2 - W)/2\n",
    "                    threshold=((100*101)/2 - norm.ppf(0.95)*np.sqrt(100*(100+1)*(200+1)/6.0))/2.0\n",
    "                ))\n",
    "            if metric_to_largerisbetter[metric]:\n",
    "                condition_to_best_methods[(alpha,samplesize)] = [methods_to_consider[x] for x in tied_top_methods]\n",
    "            else:\n",
    "                condition_to_best_methods[(alpha,samplesize)] = [methods_to_consider[x] for x in tied_worst_methods]\n",
    "    \n",
    "        table_rows = []\n",
    "        for method_name in methods_to_consider:\n",
    "            table_row = {'adapt_method_name': method_name.split(\"_\")[0]}\n",
    "            table_rows.append(table_row)\n",
    "            table_row['proposed_here'] = method_name in our_proposed_methods\n",
    "            table_row['calib_method_name'] = method_name.split(\"_\")[1].split(\"-\")[1]\n",
    "            for alpha,samplesize in set_cols:\n",
    "                \n",
    "                vals_arr = dirichletalpha_to_samplesize_to_adaptername_to_metric_to_vals[\n",
    "                                              alpha][samplesize][method_name][metric]\n",
    "                if (metric==\"delta_acc\"):\n",
    "                    vals_arr = 100*np.array(vals_arr)\n",
    "               \n",
    "                table_row[(alpha,samplesize)] = {'mean': np.mean(vals_arr),\n",
    "                                                 'std': (1.0/np.sqrt(len(vals_arr)))*np.std(vals_arr, ddof=1),\n",
    "                                                 'is_best': (method_name in condition_to_best_methods[(alpha,samplesize)])}\n",
    "\n",
    "\n",
    "        #method name, calib name, conditions...\n",
    "        the_str = \"\\\\begin{table*}\\n\\\\adjustbox{max width=\\\\textwidth}{\\\\centering\\n\\\\begin{tabular}{ | c | c | c |\"+(\"\".join([\" c |\" for x in set_cols]))+\"}\\n\"\n",
    "        the_str += \"\\\\hline\"\n",
    "        the_str += (\"\\multirow{2}{*}{\\\\begin{tabular}{c}\\\\textbf{Proposed} \\\\\\\\ \\\\textbf{Here?}\\end{tabular}}\"\n",
    "                    +\"& \\multirow{2}{*}{\\\\begin{tabular}{c}\\\\textbf{Shift} \\\\\\\\ \\\\textbf{Estimator}\\end{tabular}}\"\n",
    "                    +\"& \\multirow{2}{*}{\\\\begin{tabular}{c}\\\\textbf{Calibration} \\\\\\\\ \\\\textbf{Method}\\end{tabular}} \"\n",
    "                    +\"& \\multicolumn{\"+str(len(set_cols))+\"}{c|}{\"\n",
    "                    +metric_to_nicename[metric]+set_name+\"}\\\\\\\\ \\\\cline{4-\"+str(4+len(set_cols)-1)+\"}\\n\")\n",
    "        the_str += \"& & & \"+(\" & \".join([(\"$\\\\bm{\\\\alpha=\"+alpha+\"}$\"\n",
    "                                          if varyingparam==\"alpha\" else \"$\\\\bm{n=\"+n+\"}$\")\n",
    "                                         for (alpha,n) in set_cols]))+\"\\\\\\\\ \\\\hline\\n\"\n",
    "        for idx,table_row in enumerate(table_rows):\n",
    "            the_str += ((\"Y\" if table_row['proposed_here'] else \"N\")\n",
    "                        +\" & \"+adaptmethod_to_nicename[table_row['adapt_method_name']]\n",
    "                        +\" & \"+calibmethod_to_nicename[table_row['calib_method_name']])\n",
    "            for (alpha,samplesize) in set_cols:\n",
    "                the_str += \" & \"+(\"\\\\textbf{\" if table_row[(alpha,samplesize)]['is_best'] else \"\")\n",
    "                the_str += str(np.round(table_row[(alpha,samplesize)]['mean'],4))\n",
    "                the_str += \" $\\\\pm$ \"\n",
    "                the_str += str(np.round(table_row[(alpha,samplesize)]['std'],4))\n",
    "                the_str += (\"}\" if table_row[(alpha,samplesize)]['is_best'] else \"\")\n",
    "            \n",
    "            the_str += \"\\\\\\\\\\n\"\n",
    "            if (idx==2):\n",
    "                the_str += \"\\\\hline\\n\"\n",
    "        the_str += \"\\\\hline \\\\end{tabular}}\\n\"\n",
    "        the_str += (\"\\\\caption{\\\\textbf{\"\n",
    "            +(\"Difference from ideal weights\" if metric==\"weightdiffnorm\" else \"Improvement in \\\\%Accuracy\")\n",
    "            +\" for CIFAR10 under different \"\n",
    "            +(\"degrees of dirichlet shift $\\\\bm{\\\\alpha}$\" if varyingparam==\"alpha\" else \"values of $\\\\bm{n}$\")\n",
    "            +\"}. The value of \"\n",
    "            +(\"$\\\\alpha$ was fixed at \"+str(alpha) if varyingparam==\"n\" else \"$n$ was fixed at \"+str(n))\n",
    "            +\". Table shows mean value of \"\n",
    "            +(\"$\\\\bm{|w - \\\\hat{w}|}$\" if metric==\"weightdiffnorm\" else \"$\\\\Delta$\\\\%Accuracy\")\n",
    "            +\" for each set of 100 experiments along with the standard error.\"\n",
    "            +\" Bold numbers in a column were significantly better than\"\n",
    "            +\" non-bold numbers by a Wilcoxon signed rank test. See main text for more details.}\")\n",
    "        the_str += \"\\\\label{tab:varying\"+str(varyingparam)+\"_\"+str(metric)+\"}\\n\"\n",
    "        the_str += \"\\\\end{table*}\\n\"\n",
    "        print(the_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basepair]",
   "language": "python",
   "name": "conda-env-basepair-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
